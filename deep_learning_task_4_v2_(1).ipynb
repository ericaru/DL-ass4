{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pytorch_lightning\n"
      ],
      "metadata": {
        "id": "AieLvxMBc1lL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q neptune"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ9CTM7nsYsr",
        "outputId": "c9df63f8-3ff3-448e-bd54-eeffe73a7e18"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.9/487.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for bravado-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries and Setup\n"
      ],
      "metadata": {
        "id": "qBBvkf9m2OkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "import neptune\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "jeLiye2w2O7b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loading and Preprocessing\n"
      ],
      "metadata": {
        "id": "dS5QTW622P_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/adult.arff', comment='@', header=None)\n",
        "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
        "         'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
        "         'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
        "data.columns = columns\n",
        "\n",
        "data_clean = data.copy()\n",
        "categorical_cols = ['workclass', 'education', 'marital-status', 'occupation',\n",
        "                  'relationship', 'race', 'sex', 'native-country', 'income']\n",
        "continuous_cols = ['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week', 'education-num']\n",
        "\n",
        "for col in categorical_cols:\n",
        "   data_clean[col] = data_clean[col].str.strip()\n",
        "\n",
        "continuous_data = data_clean[continuous_cols].copy()\n",
        "categorical_data = data_clean[categorical_cols].copy()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "continuous_scaled = scaler.fit_transform(continuous_data)\n",
        "continuous_scaled = pd.DataFrame(continuous_scaled, columns=continuous_cols)\n",
        "\n",
        "categorical_encoded = pd.get_dummies(categorical_data, drop_first=False)\n",
        "final_data = pd.concat([continuous_scaled, categorical_encoded], axis=1).astype(float)\n",
        "\n",
        "X_train, X_test = train_test_split(final_data.values, test_size=0.2, random_state=42)\n",
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "X_test_tensor = torch.FloatTensor(X_test)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor)\n",
        "\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "B5H3Dg-k2Q1-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularized VAE Model\n"
      ],
      "metadata": {
        "id": "x2o-Vgx02SPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RegularizedVAE(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim=64, hidden_dims=[512, 256]):\n",
        "        super(RegularizedVAE, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        encoder_layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            encoder_layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                nn.Dropout(0.3)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "        self.fc_mu = nn.Linear(prev_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(prev_dim, latent_dim)\n",
        "\n",
        "        decoder_layers = []\n",
        "        prev_dim = latent_dim\n",
        "\n",
        "        for hidden_dim in reversed(hidden_dims):\n",
        "            decoder_layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                nn.Dropout(0.3)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        decoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        logvar = torch.clamp(logvar, min=-10, max=3)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_recon = self.decode(z)\n",
        "        return x_recon, mu, logvar"
      ],
      "metadata": {
        "id": "9iFANDnO2TGJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early Stopping Utility\n"
      ],
      "metadata": {
        "id": "MBsGiAGd2Uae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=15, min_delta=0.001, restore_best_weights=True):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "        self.best_weights = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(model)\n",
        "        elif val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            self.save_checkpoint(model)\n",
        "        else:\n",
        "            self.counter += 1\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            if self.restore_best_weights:\n",
        "                self.restore_checkpoint(model)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        self.best_weights = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "    def restore_checkpoint(self, model):\n",
        "        if self.best_weights is not None:\n",
        "            model.load_state_dict({k: v.to(model.fc_mu.weight.device) for k, v in self.best_weights.items()})"
      ],
      "metadata": {
        "id": "BbmOPIi12Vrj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VAE Loss Function\n"
      ],
      "metadata": {
        "id": "59Ma1ju_2Wap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vae_loss_with_logging(x_recon, x, mu, logvar, beta=1.0):\n",
        "    batch_size = x.size(0)\n",
        "\n",
        "    recon_loss = nn.MSELoss(reduction='sum')(x_recon, x) / batch_size\n",
        "\n",
        "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / batch_size\n",
        "\n",
        "    total_loss = recon_loss + beta * kl_loss\n",
        "\n",
        "    return total_loss, recon_loss, kl_loss"
      ],
      "metadata": {
        "id": "WDELGe6e2Xdv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VAE Training Function\n"
      ],
      "metadata": {
        "id": "fRK6NECZ2Ycp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_vae_with_early_stopping(model, train_loader, test_loader, neptune_run, epochs=200):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.7, patience=10, verbose=True\n",
        "    )\n",
        "\n",
        "    early_stopping = EarlyStopping(patience=20, min_delta=0.001)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        if epoch < epochs * 0.7:\n",
        "            beta = 0.001 * (epoch / (epochs * 0.7))\n",
        "        else:\n",
        "            beta = min(0.001 + 0.01 * ((epoch - epochs * 0.7) / (epochs * 0.3)), 0.01)\n",
        "\n",
        "        model.train()\n",
        "        train_total_loss = 0\n",
        "        train_recon_loss = 0\n",
        "        train_kl_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, (data,) in enumerate(train_loader):\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x_recon, mu, logvar = model(data)\n",
        "            loss, recon_loss, kl_loss = vae_loss_with_logging(x_recon, data, mu, logvar, beta)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_total_loss += loss.item()\n",
        "            train_recon_loss += recon_loss.item()\n",
        "            train_kl_loss += kl_loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        model.eval()\n",
        "        val_total_loss = 0\n",
        "        val_recon_loss = 0\n",
        "        val_kl_loss = 0\n",
        "        val_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, in test_loader:\n",
        "                data = data.to(device)\n",
        "                x_recon, mu, logvar = model(data)\n",
        "                loss, recon_loss, kl_loss = vae_loss_with_logging(x_recon, data, mu, logvar, beta)\n",
        "\n",
        "                val_total_loss += loss.item()\n",
        "                val_recon_loss += recon_loss.item()\n",
        "                val_kl_loss += kl_loss.item()\n",
        "                val_batches += 1\n",
        "\n",
        "        avg_train_loss = train_total_loss / num_batches\n",
        "        avg_train_recon = train_recon_loss / num_batches\n",
        "        avg_train_kl = train_kl_loss / num_batches\n",
        "\n",
        "        avg_val_loss = val_total_loss / val_batches\n",
        "        avg_val_recon = val_recon_loss / val_batches\n",
        "        avg_val_kl = val_kl_loss / val_batches\n",
        "\n",
        "        if neptune_run:\n",
        "            neptune_run[\"vae/train/total_loss\"].append(avg_train_loss)\n",
        "            neptune_run[\"vae/train/reconstruction_loss\"].append(avg_train_recon)\n",
        "            neptune_run[\"vae/train/kl_loss\"].append(avg_train_kl)\n",
        "            neptune_run[\"vae/val/total_loss\"].append(avg_val_loss)\n",
        "            neptune_run[\"vae/val/reconstruction_loss\"].append(avg_val_recon)\n",
        "            neptune_run[\"vae/val/kl_loss\"].append(avg_val_kl)\n",
        "            neptune_run[\"vae/beta\"].append(beta)\n",
        "            neptune_run[\"vae/learning_rate\"].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "            total_norm = 0\n",
        "            for p in model.parameters():\n",
        "                if p.grad is not None:\n",
        "                    param_norm = p.grad.data.norm(2)\n",
        "                    total_norm += param_norm.item() ** 2\n",
        "            total_norm = total_norm ** (1. / 2)\n",
        "            neptune_run[\"vae/gradient_norm\"].append(total_norm)\n",
        "\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}:\")\n",
        "            print(f\"  Train Loss: {avg_train_loss:.4f} (Recon: {avg_train_recon:.4f}, KL: {avg_train_kl:.4f})\")\n",
        "            print(f\"  Val Loss: {avg_val_loss:.4f} (Recon: {avg_val_recon:.4f}, KL: {avg_val_kl:.4f})\")\n",
        "            print(f\"  Beta: {beta:.6f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        if early_stopping(avg_val_loss, model):\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Q6XiU40G2ZjD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAN Generator Model\n"
      ],
      "metadata": {
        "id": "UdbFjfW72a4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RegularizedGenerator(nn.Module):\n",
        "    def __init__(self, noise_dim, latent_dim, hidden_dims=[256, 512, 256]):\n",
        "        super(RegularizedGenerator, self).__init__()\n",
        "        layers = []\n",
        "        prev_dim = noise_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                nn.Dropout(0.2)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        layers.append(nn.Linear(prev_dim, latent_dim))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "\n",
        "    def forward(self, noise):\n",
        "        return self.model(noise)"
      ],
      "metadata": {
        "id": "ARkffp3X2bdL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAN Discriminator Model\n"
      ],
      "metadata": {
        "id": "RCG1uwrE2cg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RegularizedDiscriminator(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dims=[256, 128]):\n",
        "        super(RegularizedDiscriminator, self).__init__()\n",
        "        layers = []\n",
        "        prev_dim = latent_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                nn.Dropout(0.3)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        layers.append(nn.Linear(prev_dim, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "\n",
        "    def forward(self, latent_code):\n",
        "        return self.model(latent_code)"
      ],
      "metadata": {
        "id": "dWNHsG9u2dO7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Penalty Function\n"
      ],
      "metadata": {
        "id": "dkNODOjU2ebB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_penalty(discriminator, real_data, fake_data, device, labels=None):\n",
        "    batch_size = real_data.size(0)\n",
        "    alpha = torch.rand(batch_size, 1, device=device)\n",
        "    alpha = alpha.expand_as(real_data)\n",
        "\n",
        "    interpolated = alpha * real_data + (1 - alpha) * fake_data\n",
        "    interpolated.requires_grad_(True)\n",
        "\n",
        "    if labels is not None:\n",
        "        prob_interpolated = discriminator(interpolated, labels)\n",
        "    else:\n",
        "        prob_interpolated = discriminator(interpolated)\n",
        "\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=prob_interpolated,\n",
        "        inputs=interpolated,\n",
        "        grad_outputs=torch.ones_like(prob_interpolated),\n",
        "        create_graph=True,\n",
        "        retain_graph=True\n",
        "    )[0]\n",
        "\n",
        "    gradients = gradients.view(batch_size, -1)\n",
        "    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
        "\n",
        "    return ((gradients_norm - 1) ** 2).mean()"
      ],
      "metadata": {
        "id": "2fSSPNlR2fLR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularized Latent GAN Model\n"
      ],
      "metadata": {
        "id": "7eXeEGD-2gM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RegularizedLatentGAN_Pure(nn.Module):\n",
        "    def __init__(self, vae_model, neptune_run=None, noise_dim=64, latent_dim=64, lr=1e-4):\n",
        "        super().__init__()\n",
        "        self.vae_model = vae_model\n",
        "        self.vae_model.eval()\n",
        "        for param in self.vae_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.generator = RegularizedGenerator(noise_dim, latent_dim)\n",
        "        self.discriminator = RegularizedDiscriminator(latent_dim)\n",
        "\n",
        "        self.noise_dim = noise_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.lr = lr\n",
        "        self.neptune_run = neptune_run\n",
        "\n",
        "        self.lambda_gp = 10.0\n",
        "        self.n_critic = 3\n",
        "\n",
        "    def setup_optimizers(self, device):\n",
        "        self.optimizer_g = torch.optim.Adam(self.generator.parameters(), lr=self.lr/2, betas=(0.5, 0.9), weight_decay=1e-5)\n",
        "        self.optimizer_d = torch.optim.Adam(self.discriminator.parameters(), lr=self.lr, betas=(0.5, 0.9), weight_decay=1e-5)\n",
        "\n",
        "    def training_step_manual(self, real_data, batch_idx, device):\n",
        "        batch_size = real_data.size(0)\n",
        "\n",
        "        real_data = real_data.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            real_latent, _ = self.vae_model.encode(real_data)\n",
        "            real_latent = real_latent.to(device)\n",
        "\n",
        "        self.optimizer_d.zero_grad()\n",
        "\n",
        "        real_pred = self.discriminator(real_latent)\n",
        "\n",
        "        noise = torch.randn(batch_size, self.noise_dim, device=device)\n",
        "        fake_latent = self.generator(noise)\n",
        "        fake_pred = self.discriminator(fake_latent.detach())\n",
        "\n",
        "        d_loss = -torch.mean(real_pred) + torch.mean(fake_pred)\n",
        "\n",
        "        gp = gradient_penalty(self.discriminator, real_latent, fake_latent, device)\n",
        "        d_loss += self.lambda_gp * gp\n",
        "\n",
        "        d_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), max_norm=1.0)\n",
        "        self.optimizer_d.step()\n",
        "\n",
        "        g_loss = torch.tensor(0.0, device=device)\n",
        "        if batch_idx % self.n_critic == 0:\n",
        "            self.optimizer_g.zero_grad()\n",
        "\n",
        "            noise = torch.randn(batch_size, self.noise_dim, device=device)\n",
        "            fake_latent = self.generator(noise)\n",
        "            fake_pred = self.discriminator(fake_latent)\n",
        "\n",
        "            g_loss = -torch.mean(fake_pred)\n",
        "\n",
        "            g_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.generator.parameters(), max_norm=1.0)\n",
        "            self.optimizer_g.step()\n",
        "\n",
        "        if self.neptune_run and batch_idx % 10 == 0:\n",
        "            self.neptune_run[\"gan/train/discriminator_loss\"].append(d_loss.item())\n",
        "            self.neptune_run[\"gan/train/generator_loss\"].append(g_loss.item())\n",
        "            self.neptune_run[\"gan/train/gradient_penalty\"].append(gp.item())\n",
        "            self.neptune_run[\"gan/train/real_pred_mean\"].append(torch.mean(real_pred).item())\n",
        "            self.neptune_run[\"gan/train/fake_pred_mean\"].append(torch.mean(fake_pred).item())\n",
        "\n",
        "        return {'d_loss': d_loss.item(), 'g_loss': g_loss.item(), 'gp': gp.item()}\n",
        "\n",
        "    def generate_synthetic_data(self, num_samples, device):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            batch_size = 1000\n",
        "            all_samples = []\n",
        "\n",
        "            for i in range(0, num_samples, batch_size):\n",
        "                current_batch_size = min(batch_size, num_samples - i)\n",
        "                noise = torch.randn(current_batch_size, self.noise_dim, device=device)\n",
        "                fake_latent = self.generator(noise)\n",
        "                synthetic_data = self.vae_model.decode(fake_latent)\n",
        "                all_samples.append(synthetic_data.cpu().numpy())\n",
        "\n",
        "            return np.vstack(all_samples)"
      ],
      "metadata": {
        "id": "maWQ_QBt2g7u"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditional VAE Model\n"
      ],
      "metadata": {
        "id": "O47pGRek2iQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RegularizedConditionalVAE(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim=64, num_classes=2, hidden_dims=[512, 256]):\n",
        "        super(RegularizedConditionalVAE, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        encoder_input_dim = input_dim + num_classes\n",
        "        encoder_layers = []\n",
        "        prev_dim = encoder_input_dim\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            encoder_layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                nn.Dropout(0.3)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "        self.fc_mu = nn.Linear(prev_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(prev_dim, latent_dim)\n",
        "\n",
        "        decoder_input_dim = latent_dim + num_classes\n",
        "        decoder_layers = []\n",
        "        prev_dim = decoder_input_dim\n",
        "\n",
        "        for hidden_dim in reversed(hidden_dims):\n",
        "            decoder_layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                nn.Dropout(0.3)\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        decoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "\n",
        "    def encode(self, x, labels):\n",
        "        labels_onehot = torch.nn.functional.one_hot(labels, self.num_classes).float()\n",
        "        x_labeled = torch.cat([x, labels_onehot], dim=1)\n",
        "        h = self.encoder(x_labeled)\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        logvar = torch.clamp(logvar, min=-10, max=3)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z, labels):\n",
        "        labels_onehot = torch.nn.functional.one_hot(labels, self.num_classes).float()\n",
        "        z_labeled = torch.cat([z, labels_onehot], dim=1)\n",
        "        return self.decoder(z_labeled)\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        mu, logvar = self.encode(x, labels)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_recon = self.decode(z, labels)\n",
        "        return x_recon, mu, logvar"
      ],
      "metadata": {
        "id": "DSnwcbR62i2o"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditional VAE Training Function\n"
      ],
      "metadata": {
        "id": "4HxtEwOG2kCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_conditional_vae_with_early_stopping(model, train_loader, neptune_run, epochs=150):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=10)\n",
        "    early_stopping = EarlyStopping(patience=20, min_delta=0.001)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        if epoch < epochs * 0.7:\n",
        "            beta = 0.001 * (epoch / (epochs * 0.7))\n",
        "        else:\n",
        "            beta = min(0.001 + 0.01 * ((epoch - epochs * 0.7) / (epochs * 0.3)), 0.01)\n",
        "\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        total_recon = 0\n",
        "        total_kl = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            x_recon, mu, logvar = model(data, labels)\n",
        "            loss, recon_loss, kl_loss = vae_loss_with_logging(x_recon, data, mu, logvar, beta)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_recon += recon_loss.item()\n",
        "            total_kl += kl_loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        avg_recon = total_recon / num_batches\n",
        "        avg_kl = total_kl / num_batches\n",
        "\n",
        "        if neptune_run:\n",
        "            neptune_run[\"conditional_vae/train/total_loss\"].append(avg_loss)\n",
        "            neptune_run[\"conditional_vae/train/reconstruction_loss\"].append(avg_recon)\n",
        "            neptune_run[\"conditional_vae/train/kl_loss\"].append(avg_kl)\n",
        "            neptune_run[\"conditional_vae/beta\"].append(beta)\n",
        "            neptune_run[\"conditional_vae/learning_rate\"].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        scheduler.step(avg_loss)\n",
        "\n",
        "        if (epoch + 1) % 15 == 0:\n",
        "            print(f\"Conditional VAE Epoch {epoch+1}: Loss = {avg_loss:.4f} (Recon: {avg_recon:.4f}, KL: {avg_kl:.4f}), Beta = {beta:.6f}\")\n",
        "\n",
        "        if early_stopping(avg_loss, model):\n",
        "            print(f\"Conditional VAE early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "6ueQz5Bb2lcy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditional Data Setup\n"
      ],
      "metadata": {
        "id": "aiQoqPYs2lwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "income_labels = (data_clean['income'] == '>50K').astype(int).values\n",
        "categorical_cols_no_income = [col for col in categorical_cols if col != 'income']\n",
        "categorical_data_no_income = data_clean[categorical_cols_no_income].copy()\n",
        "categorical_encoded_no_income = pd.get_dummies(categorical_data_no_income, drop_first=False)\n",
        "final_data_conditional = pd.concat([continuous_scaled, categorical_encoded_no_income], axis=1).astype(float)\n",
        "\n",
        "X_train_cond, X_test_cond, y_train_cond, y_test_cond = train_test_split(\n",
        "   final_data_conditional.values, income_labels, test_size=0.2, random_state=42, stratify=income_labels\n",
        ")\n",
        "\n",
        "X_train_cond_tensor = torch.FloatTensor(X_train_cond)\n",
        "y_train_cond_tensor = torch.LongTensor(y_train_cond)\n",
        "train_dataset_cond = TensorDataset(X_train_cond_tensor, y_train_cond_tensor)\n",
        "train_loader_cond = DataLoader(train_dataset_cond, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "input_dim_cond = X_train_cond.shape[1]\n",
        "num_classes = 2"
      ],
      "metadata": {
        "id": "jp7-fM6t2mXt"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neptune Runs Initialization\n"
      ],
      "metadata": {
        "id": "yjWxMnaR2nlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_vae = neptune.init_run(\n",
        "   project=\"alon.sadot02/DeepLearning-task4\",\n",
        "   api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI4ODJjMTE2MC0wNTk5LTRlOGYtOWMxMC0zNTg5NzdkYzlkZjcifQ==\",\n",
        "   tags=[\"regularized-VAE\", \"early-stopping\", \"adult-dataset\"],\n",
        "   name=\"Regularized-VAE-Training\"\n",
        ")\n",
        "\n",
        "run_gan = neptune.init_run(\n",
        "   project=\"alon.sadot02/DeepLearning-task4\",\n",
        "   api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI4ODJjMTE2MC0wNTk5LTRlOGYtOWMxMC0zNTg5NzdkYzlkZjcifQ==\",\n",
        "   tags=[\"regularized-GAN\", \"early-stopping\", \"WGAN-GP\", \"adult-dataset\"],\n",
        "   name=\"Regularized-GAN-Training\"\n",
        ")\n",
        "\n",
        "run_cgan = neptune.init_run(\n",
        "  project=\"alon.sadot02/DeepLearning-task4\",\n",
        "  api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI4ODJjMTE2MC0wNTk5LTRlOGYtOWMxMC0zNTg5NzdkYzlkZjcifQ==\",\n",
        "  tags=[\"regularized-cGAN\", \"early-stopping\", \"conditional\", \"adult-dataset\"],\n",
        "  name=\"Regularized-Conditional-GAN-Training\"\n",
        ")"
      ],
      "metadata": {
        "id": "s7JvHVFA2oTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42ec40b5-cdab-4071-f013-8b4876bb6aee"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/alon.sadot02/DeepLearning-task4/e/DEEP1-100\n",
            "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/alon.sadot02/DeepLearning-task4/e/DEEP1-101\n",
            "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/alon.sadot02/DeepLearning-task4/e/DEEP1-102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters Logging\n"
      ],
      "metadata": {
        "id": "gdJrwslm2pqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_vae[\"hyperparameters\"] = {\n",
        "   \"latent_dim\": 64,\n",
        "   \"hidden_dims\": [512, 256],\n",
        "   \"learning_rate\": 5e-4,\n",
        "   \"weight_decay\": 1e-4,\n",
        "   \"dropout\": 0.3,\n",
        "   \"early_stopping_patience\": 20,\n",
        "   \"batch_size\": batch_size,\n",
        "   \"max_epochs\": 200,\n",
        "   \"beta_max\": 0.01\n",
        "}\n",
        "\n",
        "run_gan[\"hyperparameters\"] = {\n",
        "   \"noise_dim\": 64,\n",
        "   \"latent_dim\": 64,\n",
        "   \"generator_hidden\": [256, 512, 256],\n",
        "   \"discriminator_hidden\": [256, 128],\n",
        "   \"learning_rate_g\": 5e-5,\n",
        "   \"learning_rate_d\": 1e-4,\n",
        "   \"weight_decay\": 1e-5,\n",
        "   \"dropout\": 0.2,\n",
        "   \"lambda_gp\": 10.0,\n",
        "   \"n_critic\": 3,\n",
        "   \"batch_size\": batch_size,\n",
        "   \"max_epochs\": 150\n",
        "}"
      ],
      "metadata": {
        "id": "4UyHbZIn2qNC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training\n"
      ],
      "metadata": {
        "id": "14D1Yyb-2sm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Regularized VAE with Early Stopping...\")\n",
        "regularized_vae = RegularizedVAE(input_dim, latent_dim=64).to(device)\n",
        "regularized_vae = train_vae_with_early_stopping(regularized_vae, train_loader, test_loader, run_vae, epochs=200)\n",
        "\n",
        "print(\"Training Regularized Conditional VAE with Early Stopping...\")\n",
        "regularized_conditional_vae = RegularizedConditionalVAE(input_dim_cond, latent_dim=64, num_classes=2).to(device)\n",
        "regularized_conditional_vae = train_conditional_vae_with_early_stopping(regularized_conditional_vae, train_loader_cond, run_cgan, epochs=150)\n",
        "\n",
        "print(\"Training Regularized GANs with Early Stopping...\")\n",
        "\n",
        "regularized_gan = RegularizedLatentGAN_Pure(\n",
        "    vae_model=regularized_vae,\n",
        "    neptune_run=run_gan,\n",
        "    noise_dim=64,\n",
        "    latent_dim=64,\n",
        "    lr=1e-4\n",
        ")\n",
        "\n",
        "print(\"Moving models to device...\")\n",
        "regularized_vae = regularized_vae.to(device)\n",
        "regularized_conditional_vae = regularized_conditional_vae.to(device)"
      ],
      "metadata": {
        "id": "Mt-72Sn52s3-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e976542-e399-40de-92c9-9c87e1beabf4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Regularized VAE with Early Stopping...\n",
            "Epoch 10/200:\n",
            "  Train Loss: 2.3635 (Recon: 2.3437, KL: 307.8863)\n",
            "  Val Loss: 1.1809 (Recon: 1.1628, KL: 282.1476)\n",
            "  Beta: 0.000064, LR: 0.000500\n",
            "Epoch 20/200:\n",
            "  Train Loss: 1.7174 (Recon: 1.6846, KL: 241.2368)\n",
            "  Val Loss: 0.8437 (Recon: 0.8136, KL: 221.6266)\n",
            "  Beta: 0.000136, LR: 0.000500\n",
            "Epoch 30/200:\n",
            "  Train Loss: 1.4690 (Recon: 1.4288, KL: 193.9276)\n",
            "  Val Loss: 0.6401 (Recon: 0.6030, KL: 179.2718)\n",
            "  Beta: 0.000207, LR: 0.000500\n",
            "Epoch 40/200:\n",
            "  Train Loss: 1.3244 (Recon: 1.2762, KL: 172.9607)\n",
            "  Val Loss: 0.5234 (Recon: 0.4784, KL: 161.7161)\n",
            "  Beta: 0.000279, LR: 0.000500\n",
            "Epoch 50/200:\n",
            "  Train Loss: 1.2383 (Recon: 1.1823, KL: 159.9030)\n",
            "  Val Loss: 0.4489 (Recon: 0.3973, KL: 147.3377)\n",
            "  Beta: 0.000350, LR: 0.000500\n",
            "Epoch 60/200:\n",
            "  Train Loss: 1.2041 (Recon: 1.1403, KL: 151.4395)\n",
            "  Val Loss: 0.4445 (Recon: 0.3840, KL: 143.6146)\n",
            "  Beta: 0.000421, LR: 0.000500\n",
            "Epoch 70/200:\n",
            "  Train Loss: 1.1748 (Recon: 1.1032, KL: 145.3891)\n",
            "  Val Loss: 0.4089 (Recon: 0.3422, KL: 135.3240)\n",
            "  Beta: 0.000493, LR: 0.000500\n",
            "Epoch 80/200:\n",
            "  Train Loss: 1.1637 (Recon: 1.0848, KL: 139.8059)\n",
            "  Val Loss: 0.4332 (Recon: 0.3578, KL: 133.5041)\n",
            "  Beta: 0.000564, LR: 0.000500\n",
            "Epoch 90/200:\n",
            "  Train Loss: 1.1611 (Recon: 1.0756, KL: 134.4624)\n",
            "  Val Loss: 0.4044 (Recon: 0.3229, KL: 128.2594)\n",
            "  Beta: 0.000636, LR: 0.000500\n",
            "Epoch 100/200:\n",
            "  Train Loss: 1.1315 (Recon: 1.0396, KL: 130.0372)\n",
            "  Val Loss: 0.3862 (Recon: 0.2985, KL: 123.9232)\n",
            "  Beta: 0.000707, LR: 0.000350\n",
            "Epoch 110/200:\n",
            "  Train Loss: 1.1223 (Recon: 1.0244, KL: 125.7475)\n",
            "  Val Loss: 0.3861 (Recon: 0.2933, KL: 119.1443)\n",
            "  Beta: 0.000779, LR: 0.000350\n",
            "Epoch 120/200:\n",
            "  Train Loss: 1.1139 (Recon: 1.0109, KL: 121.1814)\n",
            "  Val Loss: 0.3836 (Recon: 0.2861, KL: 114.7973)\n",
            "  Beta: 0.000850, LR: 0.000245\n",
            "Epoch 130/200:\n",
            "  Train Loss: 1.1101 (Recon: 1.0014, KL: 118.0086)\n",
            "  Val Loss: 0.3898 (Recon: 0.2871, KL: 111.3583)\n",
            "  Beta: 0.000921, LR: 0.000171\n",
            "Early stopping at epoch 139\n",
            "Training Regularized Conditional VAE with Early Stopping...\n",
            "Conditional VAE Epoch 15: Loss = 1.8574 (Recon: 1.8225, KL: 261.6579), Beta = 0.000133\n",
            "Conditional VAE Epoch 30: Loss = 1.3898 (Recon: 1.3404, KL: 178.8540), Beta = 0.000276\n",
            "Conditional VAE Epoch 45: Loss = 1.2186 (Recon: 1.1552, KL: 151.2994), Beta = 0.000419\n",
            "Conditional VAE Epoch 60: Loss = 1.1600 (Recon: 1.0822, KL: 138.5652), Beta = 0.000562\n",
            "Conditional VAE Epoch 75: Loss = 1.1236 (Recon: 1.0321, KL: 129.7741), Beta = 0.000705\n",
            "Conditional VAE Epoch 90: Loss = 1.1102 (Recon: 1.0069, KL: 121.9176), Beta = 0.000848\n",
            "Conditional VAE Epoch 105: Loss = 1.1081 (Recon: 0.9950, KL: 114.1977), Beta = 0.000990\n",
            "Conditional VAE early stopping at epoch 116\n",
            "Training Regularized GANs with Early Stopping...\n",
            "Moving models to device...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAN Training Function with Early Stopping\n"
      ],
      "metadata": {
        "id": "CpAfDTGB9QrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan_with_early_stopping_fixed(gan_model, train_loader, device, epochs=150):\n",
        "    gan_model = gan_model.to(device)\n",
        "    gan_model.setup_optimizers(device)\n",
        "\n",
        "    d_losses = []\n",
        "    g_losses = []\n",
        "    best_combined_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    patience = 30\n",
        "\n",
        "    print(f\"Training GAN on device: {device}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_d_losses = []\n",
        "        epoch_g_losses = []\n",
        "\n",
        "        gan_model.train()\n",
        "\n",
        "        for batch_idx, (data,) in enumerate(train_loader):\n",
        "            result = gan_model.training_step_manual(data, batch_idx, device)\n",
        "            epoch_d_losses.append(result['d_loss'])\n",
        "            if result['g_loss'] != 0:\n",
        "                epoch_g_losses.append(result['g_loss'])\n",
        "\n",
        "        avg_d_loss = np.mean(epoch_d_losses)\n",
        "        avg_g_loss = np.mean(epoch_g_losses) if epoch_g_losses else 0.0\n",
        "\n",
        "        d_losses.append(avg_d_loss)\n",
        "        g_losses.append(avg_g_loss)\n",
        "\n",
        "        combined_loss = abs(avg_d_loss) + abs(avg_g_loss)\n",
        "\n",
        "        if gan_model.neptune_run:\n",
        "            gan_model.neptune_run[\"gan/epoch/discriminator_loss\"].append(avg_d_loss)\n",
        "            gan_model.neptune_run[\"gan/epoch/generator_loss\"].append(avg_g_loss)\n",
        "            gan_model.neptune_run[\"gan/epoch/combined_loss\"].append(combined_loss)\n",
        "\n",
        "        if combined_loss < best_combined_loss:\n",
        "            best_combined_loss = combined_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(gan_model.state_dict(), 'best_gan_model.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"GAN Epoch {epoch+1}: D_loss = {avg_d_loss:.4f}, G_loss = {avg_g_loss:.4f}, Combined = {combined_loss:.4f}\")\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"GAN early stopping at epoch {epoch+1}\")\n",
        "            gan_model.load_state_dict(torch.load('best_gan_model.pth'))\n",
        "            break\n",
        "\n",
        "    return gan_model"
      ],
      "metadata": {
        "id": "mX-j9ip42uaR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAN Training Execution\n"
      ],
      "metadata": {
        "id": "WK5G-ecR9SNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_gan = Trainer(\n",
        "   max_epochs=1,\n",
        "   accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
        "   devices=1,\n",
        "   logger=False,\n",
        "   enable_checkpointing=False,\n",
        "   enable_progress_bar=False\n",
        ")\n",
        "\n",
        "regularized_gan = train_gan_with_early_stopping_fixed(regularized_gan, train_loader, device, epochs=150)"
      ],
      "metadata": {
        "id": "8bQScev_9S5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34f35431-e2a4-41ea-907e-5fae2aa5b24b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training GAN on device: cuda\n",
            "GAN Epoch 10: D_loss = -1.3229, G_loss = 1.2850, Combined = 2.6079\n",
            "GAN Epoch 20: D_loss = -1.4638, G_loss = 1.6194, Combined = 3.0831\n",
            "GAN Epoch 30: D_loss = -1.3088, G_loss = 1.3141, Combined = 2.6229\n",
            "GAN early stopping at epoch 37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditional GAN Models\n"
      ],
      "metadata": {
        "id": "O_gIsHPi9T0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RegularizedConditionalGenerator(nn.Module):\n",
        "   def __init__(self, noise_dim, latent_dim, num_classes, hidden_dims=[256, 512, 256]):\n",
        "       super(RegularizedConditionalGenerator, self).__init__()\n",
        "       self.num_classes = num_classes\n",
        "       input_dim = noise_dim + num_classes\n",
        "\n",
        "       layers = []\n",
        "       prev_dim = input_dim\n",
        "       for hidden_dim in hidden_dims:\n",
        "           layers.extend([\n",
        "               nn.Linear(prev_dim, hidden_dim),\n",
        "               nn.BatchNorm1d(hidden_dim),\n",
        "               nn.LeakyReLU(0.2),\n",
        "               nn.Dropout(0.2)\n",
        "           ])\n",
        "           prev_dim = hidden_dim\n",
        "\n",
        "       layers.append(nn.Linear(prev_dim, latent_dim))\n",
        "       self.model = nn.Sequential(*layers)\n",
        "       self.apply(self._init_weights)\n",
        "\n",
        "   def _init_weights(self, module):\n",
        "       if isinstance(module, nn.Linear):\n",
        "           nn.init.xavier_uniform_(module.weight)\n",
        "           if module.bias is not None:\n",
        "               nn.init.constant_(module.bias, 0)\n",
        "\n",
        "   def forward(self, noise, labels):\n",
        "       labels_onehot = torch.nn.functional.one_hot(labels, self.num_classes).float()\n",
        "       input_tensor = torch.cat([noise, labels_onehot], dim=1)\n",
        "       return self.model(input_tensor)\n",
        "\n",
        "class RegularizedConditionalDiscriminator(nn.Module):\n",
        "   def __init__(self, latent_dim, num_classes, hidden_dims=[256, 128]):\n",
        "       super(RegularizedConditionalDiscriminator, self).__init__()\n",
        "       self.num_classes = num_classes\n",
        "       input_dim = latent_dim + num_classes\n",
        "\n",
        "       layers = []\n",
        "       prev_dim = input_dim\n",
        "       for hidden_dim in hidden_dims:\n",
        "           layers.extend([\n",
        "               nn.Linear(prev_dim, hidden_dim),\n",
        "               nn.LeakyReLU(0.2),\n",
        "               nn.Dropout(0.3)\n",
        "           ])\n",
        "           prev_dim = hidden_dim\n",
        "\n",
        "       layers.append(nn.Linear(prev_dim, 1))\n",
        "       self.model = nn.Sequential(*layers)\n",
        "       self.apply(self._init_weights)\n",
        "\n",
        "   def _init_weights(self, module):\n",
        "       if isinstance(module, nn.Linear):\n",
        "           nn.init.xavier_uniform_(module.weight)\n",
        "           if module.bias is not None:\n",
        "               nn.init.constant_(module.bias, 0)\n",
        "\n",
        "   def forward(self, latent_code, labels):\n",
        "       labels_onehot = torch.nn.functional.one_hot(labels, self.num_classes).float()\n",
        "       input_tensor = torch.cat([latent_code, labels_onehot], dim=1)\n",
        "       return self.model(input_tensor)"
      ],
      "metadata": {
        "id": "DrDk4-ce9UeO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditional Latent GAN Model\n"
      ],
      "metadata": {
        "id": "7eTZ178o9Vly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RegularizedConditionalLatentGAN_Pure(nn.Module):\n",
        "    def __init__(self, conditional_vae_model, neptune_run=None, noise_dim=64, latent_dim=64, num_classes=2, lr=1e-4):\n",
        "        super().__init__()\n",
        "        self.conditional_vae_model = conditional_vae_model\n",
        "        self.conditional_vae_model.eval()\n",
        "        for param in self.conditional_vae_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.generator = RegularizedConditionalGenerator(noise_dim, latent_dim, num_classes)\n",
        "        self.discriminator = RegularizedConditionalDiscriminator(latent_dim, num_classes)\n",
        "\n",
        "        self.noise_dim = noise_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.lr = lr\n",
        "        self.neptune_run = neptune_run\n",
        "\n",
        "        self.lambda_gp = 10.0\n",
        "        self.n_critic = 3\n",
        "\n",
        "    def setup_optimizers(self, device):\n",
        "        self.optimizer_g = torch.optim.Adam(self.generator.parameters(), lr=self.lr/2, betas=(0.5, 0.9), weight_decay=1e-5)\n",
        "        self.optimizer_d = torch.optim.Adam(self.discriminator.parameters(), lr=self.lr, betas=(0.5, 0.9), weight_decay=1e-5)\n",
        "\n",
        "    def training_step_manual(self, real_data, labels, batch_idx, device):\n",
        "        batch_size = real_data.size(0)\n",
        "\n",
        "        real_data = real_data.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            real_latent, _ = self.conditional_vae_model.encode(real_data, labels)\n",
        "            real_latent = real_latent.to(device)\n",
        "\n",
        "        self.optimizer_d.zero_grad()\n",
        "\n",
        "        real_pred = self.discriminator(real_latent, labels)\n",
        "\n",
        "        noise = torch.randn(batch_size, self.noise_dim, device=device)\n",
        "        fake_latent = self.generator(noise, labels)\n",
        "        fake_pred = self.discriminator(fake_latent.detach(), labels)\n",
        "\n",
        "        d_loss = -torch.mean(real_pred) + torch.mean(fake_pred)\n",
        "\n",
        "        gp = gradient_penalty(self.discriminator, real_latent, fake_latent, device, labels)\n",
        "        d_loss += self.lambda_gp * gp\n",
        "\n",
        "        d_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), max_norm=1.0)\n",
        "        self.optimizer_d.step()\n",
        "\n",
        "        g_loss = torch.tensor(0.0, device=device)\n",
        "        if batch_idx % self.n_critic == 0:\n",
        "            self.optimizer_g.zero_grad()\n",
        "\n",
        "            noise = torch.randn(batch_size, self.noise_dim, device=device)\n",
        "            fake_latent = self.generator(noise, labels)\n",
        "            fake_pred = self.discriminator(fake_latent, labels)\n",
        "\n",
        "            g_loss = -torch.mean(fake_pred)\n",
        "\n",
        "            g_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.generator.parameters(), max_norm=1.0)\n",
        "            self.optimizer_g.step()\n",
        "\n",
        "        if self.neptune_run and batch_idx % 10 == 0:\n",
        "            self.neptune_run[\"conditional_gan/train/discriminator_loss\"].append(d_loss.item())\n",
        "            self.neptune_run[\"conditional_gan/train/generator_loss\"].append(g_loss.item())\n",
        "            self.neptune_run[\"conditional_gan/train/gradient_penalty\"].append(gp.item())\n",
        "\n",
        "        return {'d_loss': d_loss.item(), 'g_loss': g_loss.item()}\n",
        "\n",
        "    def generate_conditional_synthetic_data(self, num_samples, target_labels, device):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            batch_size = 1000\n",
        "            all_samples = []\n",
        "\n",
        "            for i in range(0, num_samples, batch_size):\n",
        "                current_batch_size = min(batch_size, num_samples - i)\n",
        "                start_idx = i\n",
        "                end_idx = min(i + current_batch_size, len(target_labels))\n",
        "\n",
        "                noise = torch.randn(end_idx - start_idx, self.noise_dim, device=device)\n",
        "                batch_labels = torch.tensor(target_labels[start_idx:end_idx], device=device)\n",
        "                fake_latent = self.generator(noise, batch_labels)\n",
        "                synthetic_data = self.conditional_vae_model.decode(fake_latent, batch_labels)\n",
        "                all_samples.append(synthetic_data.cpu().numpy())\n",
        "\n",
        "            return np.vstack(all_samples)"
      ],
      "metadata": {
        "id": "5EyAvQtE9WkS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditional GAN Training Function\n"
      ],
      "metadata": {
        "id": "AHwj6B0y9W3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_conditional_gan_with_early_stopping_fixed(cgan_model, train_loader, device, epochs=150):\n",
        "    cgan_model = cgan_model.to(device)\n",
        "    cgan_model.setup_optimizers(device)\n",
        "\n",
        "    d_losses = []\n",
        "    g_losses = []\n",
        "    best_combined_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    patience = 30\n",
        "\n",
        "    print(f\"Training Conditional GAN on device: {device}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_d_losses = []\n",
        "        epoch_g_losses = []\n",
        "\n",
        "        cgan_model.train()\n",
        "\n",
        "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "            result = cgan_model.training_step_manual(data, labels, batch_idx, device)\n",
        "            epoch_d_losses.append(result['d_loss'])\n",
        "            if result['g_loss'] != 0:\n",
        "                epoch_g_losses.append(result['g_loss'])\n",
        "\n",
        "        avg_d_loss = np.mean(epoch_d_losses)\n",
        "        avg_g_loss = np.mean(epoch_g_losses) if epoch_g_losses else 0.0\n",
        "\n",
        "        d_losses.append(avg_d_loss)\n",
        "        g_losses.append(avg_g_loss)\n",
        "\n",
        "        combined_loss = abs(avg_d_loss) + abs(avg_g_loss)\n",
        "\n",
        "        if cgan_model.neptune_run:\n",
        "            cgan_model.neptune_run[\"conditional_gan/epoch/discriminator_loss\"].append(avg_d_loss)\n",
        "            cgan_model.neptune_run[\"conditional_gan/epoch/generator_loss\"].append(avg_g_loss)\n",
        "            cgan_model.neptune_run[\"conditional_gan/epoch/combined_loss\"].append(combined_loss)\n",
        "\n",
        "        if combined_loss < best_combined_loss:\n",
        "            best_combined_loss = combined_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(cgan_model.state_dict(), 'best_cgan_model.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Conditional GAN Epoch {epoch+1}: D_loss = {avg_d_loss:.4f}, G_loss = {avg_g_loss:.4f}\")\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Conditional GAN early stopping at epoch {epoch+1}\")\n",
        "            cgan_model.load_state_dict(torch.load('best_cgan_model.pth'))\n",
        "            break\n",
        "\n",
        "    return cgan_model"
      ],
      "metadata": {
        "id": "iveQtH0Q9YCH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditional GAN Training Execution\n"
      ],
      "metadata": {
        "id": "dYFXrvXA9ZCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regularized_cgan = RegularizedConditionalLatentGAN_Pure(\n",
        "    conditional_vae_model=regularized_conditional_vae,\n",
        "    neptune_run=run_cgan,\n",
        "    noise_dim=64,\n",
        "    latent_dim=64,\n",
        "    num_classes=2,\n",
        "    lr=1e-4\n",
        ")\n",
        "\n",
        "regularized_cgan = train_conditional_gan_with_early_stopping_fixed(regularized_cgan, train_loader_cond, device, epochs=150)"
      ],
      "metadata": {
        "id": "GErvqOwt9aKz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22247d64-4f78-40f2-e0c9-7a78cb02d7b1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Conditional GAN on device: cuda\n",
            "Conditional GAN Epoch 10: D_loss = -1.4239, G_loss = 1.7963\n",
            "Conditional GAN Epoch 20: D_loss = -1.3716, G_loss = 1.8531\n",
            "Conditional GAN Epoch 30: D_loss = -1.2162, G_loss = 1.6927\n",
            "Conditional GAN early stopping at epoch 37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Synthetic Data Generation\n"
      ],
      "metadata": {
        "id": "-TsTYcRz9oo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating synthetic data from regularized models...\")\n",
        "\n",
        "def generate_regularized_synthetic_datasets(gan_model, cgan_model, train_size, y_train_cond):\n",
        "   regular_synthetic_data = gan_model.generate_synthetic_data(train_size, device)\n",
        "\n",
        "   class_counts = np.bincount(y_train_cond)\n",
        "   low_income_count = class_counts[0]\n",
        "   high_income_count = class_counts[1]\n",
        "\n",
        "   low_income_labels = [0] * low_income_count\n",
        "   high_income_labels = [1] * high_income_count\n",
        "\n",
        "   low_income_synthetic = cgan_model.generate_conditional_synthetic_data(low_income_count, low_income_labels, device)\n",
        "   high_income_synthetic = cgan_model.generate_conditional_synthetic_data(high_income_count, high_income_labels, device)\n",
        "\n",
        "   synthetic_low_labels = np.zeros(low_income_count, dtype=int)\n",
        "   synthetic_high_labels = np.ones(high_income_count, dtype=int)\n",
        "\n",
        "   conditional_synthetic_data = np.vstack([low_income_synthetic, high_income_synthetic])\n",
        "   conditional_synthetic_labels = np.concatenate([synthetic_low_labels, synthetic_high_labels])\n",
        "\n",
        "   shuffle_indices = np.random.permutation(len(conditional_synthetic_data))\n",
        "   conditional_synthetic_data = conditional_synthetic_data[shuffle_indices]\n",
        "   conditional_synthetic_labels = conditional_synthetic_labels[shuffle_indices]\n",
        "\n",
        "   return regular_synthetic_data, conditional_synthetic_data, conditional_synthetic_labels\n",
        "\n",
        "regularized_regular_synthetic, regularized_conditional_synthetic, regularized_conditional_labels = generate_regularized_synthetic_datasets(\n",
        "  regularized_gan, regularized_cgan, len(X_train), y_train_cond\n",
        ")"
      ],
      "metadata": {
        "id": "oIzZul2V9pdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac10fe90-0627-48c0-def7-539ca6ade8fd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating synthetic data from regularized models...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Format Conversion Functions\n"
      ],
      "metadata": {
        "id": "Y6bxyuXj9qLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_original_format(synthetic_array, scaler, categorical_cols, continuous_cols, final_data):\n",
        "  continuous_synthetic = synthetic_array[:, :len(continuous_cols)]\n",
        "  categorical_synthetic = synthetic_array[:, len(continuous_cols):]\n",
        "\n",
        "  continuous_original = scaler.inverse_transform(continuous_synthetic)\n",
        "  continuous_df = pd.DataFrame(continuous_original, columns=continuous_cols)\n",
        "\n",
        "  categorical_encoded_cols = final_data.columns[len(continuous_cols):]\n",
        "  categorical_df_encoded = pd.DataFrame(categorical_synthetic, columns=categorical_encoded_cols)\n",
        "\n",
        "  categorical_df = pd.DataFrame()\n",
        "  for cat_col in categorical_cols:\n",
        "      cat_columns = [col for col in categorical_encoded_cols if col.startswith(f'{cat_col}_')]\n",
        "      if cat_columns:\n",
        "          cat_data = categorical_df_encoded[cat_columns].values\n",
        "          cat_indices = np.argmax(cat_data, axis=1)\n",
        "          categories = [col.replace(f'{cat_col}_', '') for col in cat_columns]\n",
        "          categorical_values = [categories[i] for i in cat_indices]\n",
        "          categorical_df[cat_col] = categorical_values\n",
        "\n",
        "  result_df = pd.concat([continuous_df, categorical_df], axis=1)\n",
        "  all_columns = continuous_cols + categorical_cols\n",
        "  result_df = result_df[all_columns]\n",
        "\n",
        "  for col in continuous_cols:\n",
        "      if col in ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']:\n",
        "          result_df[col] = result_df[col].clip(lower=0).round().astype(int)\n",
        "\n",
        "  return result_df\n",
        "\n",
        "def convert_conditional_to_original_format(synthetic_array, labels, scaler, categorical_cols, continuous_cols, final_data_conditional):\n",
        "  continuous_synthetic = synthetic_array[:, :len(continuous_cols)]\n",
        "  categorical_synthetic = synthetic_array[:, len(continuous_cols):]\n",
        "\n",
        "  continuous_original = scaler.inverse_transform(continuous_synthetic)\n",
        "  continuous_df = pd.DataFrame(continuous_original, columns=continuous_cols)\n",
        "\n",
        "  categorical_cols_no_income = [col for col in categorical_cols if col != 'income']\n",
        "  categorical_encoded_cols_no_income = [col for col in final_data_conditional.columns[len(continuous_cols):]]\n",
        "  categorical_df_encoded = pd.DataFrame(categorical_synthetic, columns=categorical_encoded_cols_no_income)\n",
        "\n",
        "  categorical_df = pd.DataFrame()\n",
        "  for cat_col in categorical_cols_no_income:\n",
        "      cat_columns = [col for col in categorical_encoded_cols_no_income if col.startswith(f'{cat_col}_')]\n",
        "      if cat_columns:\n",
        "          cat_data = categorical_df_encoded[cat_columns].values\n",
        "          cat_indices = np.argmax(cat_data, axis=1)\n",
        "          categories = [col.replace(f'{cat_col}_', '') for col in cat_columns]\n",
        "          categorical_values = [categories[i] for i in cat_indices]\n",
        "          categorical_df[cat_col] = categorical_values\n",
        "\n",
        "  income_values = ['<=50K' if label == 0 else '>50K' for label in labels]\n",
        "  categorical_df['income'] = income_values\n",
        "\n",
        "  result_df = pd.concat([continuous_df, categorical_df], axis=1)\n",
        "  all_columns = continuous_cols + categorical_cols\n",
        "  result_df = result_df[all_columns]\n",
        "\n",
        "  for col in continuous_cols:\n",
        "      if col in ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']:\n",
        "          result_df[col] = result_df[col].clip(lower=0).round().astype(int)\n",
        "\n",
        "  return result_df"
      ],
      "metadata": {
        "id": "bT8SPhXH9rER"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Conversion and Constraints\n"
      ],
      "metadata": {
        "id": "InZu_Mpn9rau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regularized_dataset1_original = convert_to_original_format(regularized_regular_synthetic, scaler, categorical_cols, continuous_cols, final_data)\n",
        "regularized_dataset2_original = convert_conditional_to_original_format(regularized_conditional_synthetic, regularized_conditional_labels, scaler, categorical_cols, continuous_cols, final_data_conditional)\n",
        "\n",
        "def minimal_domain_constraints(synthetic_data):\n",
        "   processed = synthetic_data.copy()\n",
        "\n",
        "   if 'age' in processed.columns:\n",
        "       processed['age'] = processed['age'].clip(17, 90)\n",
        "\n",
        "   if 'hours-per-week' in processed.columns:\n",
        "       processed['hours-per-week'] = processed['hours-per-week'].clip(1, 99)\n",
        "\n",
        "   financial_cols = ['fnlwgt', 'capital-gain', 'capital-loss']\n",
        "   for col in financial_cols:\n",
        "       if col in processed.columns:\n",
        "           processed[col] = processed[col].clip(lower=0)\n",
        "\n",
        "   int_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
        "   for col in int_cols:\n",
        "       if col in processed.columns:\n",
        "           processed[col] = processed[col].round().astype(int)\n",
        "\n",
        "   return processed\n",
        "\n",
        "regularized_dataset1_final = minimal_domain_constraints(regularized_dataset1_original)\n",
        "regularized_dataset2_final = minimal_domain_constraints(regularized_dataset2_original)\n",
        "\n",
        "regularized_dataset1_final.to_csv(\"regularized_synthetic_adult_regular_gan.csv\", index=False)\n",
        "regularized_dataset2_final.to_csv(\"regularized_synthetic_adult_conditional_gan.csv\", index=False)\n",
        "data_clean.to_csv(\"original_adult_reference.csv\", index=False)"
      ],
      "metadata": {
        "id": "9tiKvKx99st1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Functions\n"
      ],
      "metadata": {
        "id": "n1vdG2zU9s-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detection_evaluation_4fold(original_training_data, synthetic_data):\n",
        "  from sklearn.model_selection import KFold\n",
        "\n",
        "  def prepare_ml_data(df):\n",
        "      df_ml = df.copy()\n",
        "      for col in df.select_dtypes(include=['object']).columns:\n",
        "          le = LabelEncoder()\n",
        "          df_ml[col] = le.fit_transform(df_ml[col].astype(str))\n",
        "      return df_ml\n",
        "\n",
        "  original_ml = prepare_ml_data(original_training_data)\n",
        "  synthetic_ml = prepare_ml_data(synthetic_data)\n",
        "\n",
        "  min_size = min(len(original_ml), len(synthetic_ml))\n",
        "  real_sample = original_ml.sample(n=min_size, random_state=42).reset_index(drop=True)\n",
        "  synthetic_sample = synthetic_ml.sample(n=min_size, random_state=42).reset_index(drop=True)\n",
        "\n",
        "  kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "  real_folds = []\n",
        "  synthetic_folds = []\n",
        "\n",
        "  for train_idx, test_idx in kf.split(real_sample):\n",
        "      real_folds.append({\n",
        "          'train': real_sample.iloc[train_idx],\n",
        "          'test': real_sample.iloc[test_idx]\n",
        "      })\n",
        "\n",
        "  for train_idx, test_idx in kf.split(synthetic_sample):\n",
        "      synthetic_folds.append({\n",
        "          'train': synthetic_sample.iloc[train_idx],\n",
        "          'test': synthetic_sample.iloc[test_idx]\n",
        "      })\n",
        "\n",
        "  auc_scores = []\n",
        "\n",
        "  for fold in range(4):\n",
        "      real_train_folds = [real_folds[i]['train'] for i in range(4) if i != fold]\n",
        "      synthetic_train_folds = [synthetic_folds[i]['train'] for i in range(4) if i != fold]\n",
        "\n",
        "      real_train = pd.concat(real_train_folds, ignore_index=True)\n",
        "      synthetic_train = pd.concat(synthetic_train_folds, ignore_index=True)\n",
        "\n",
        "      real_test = real_folds[fold]['test']\n",
        "      synthetic_test = synthetic_folds[fold]['test']\n",
        "\n",
        "      X_train = pd.concat([real_train, synthetic_train], ignore_index=True)\n",
        "      y_train = np.concatenate([np.zeros(len(real_train)), np.ones(len(synthetic_train))])\n",
        "\n",
        "      X_test = pd.concat([real_test, synthetic_test], ignore_index=True)\n",
        "      y_test = np.concatenate([np.zeros(len(real_test)), np.ones(len(synthetic_test))])\n",
        "\n",
        "      train_shuffle_idx = np.random.RandomState(42 + fold).permutation(len(X_train))\n",
        "      test_shuffle_idx = np.random.RandomState(42 + fold).permutation(len(X_test))\n",
        "\n",
        "      X_train = X_train.iloc[train_shuffle_idx].reset_index(drop=True)\n",
        "      y_train = y_train[train_shuffle_idx]\n",
        "      X_test = X_test.iloc[test_shuffle_idx].reset_index(drop=True)\n",
        "      y_test = y_test[test_shuffle_idx]\n",
        "\n",
        "      rf = RandomForestClassifier(n_estimators=100, random_state=42 + fold, n_jobs=-1)\n",
        "      rf.fit(X_train, y_train)\n",
        "\n",
        "      y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
        "      auc = roc_auc_score(y_test, y_pred_proba)\n",
        "      auc_scores.append(auc)\n",
        "\n",
        "  mean_auc = np.mean(auc_scores)\n",
        "  return mean_auc, auc_scores\n",
        "\n",
        "def efficacy_evaluation(original_train_data, original_test_data, synthetic_data):\n",
        "    def prepare_ml_data_consistent(train_df, test_df, synthetic_df):\n",
        "        all_data = pd.concat([train_df, test_df, synthetic_df], ignore_index=True)\n",
        "        categorical_cols_eval = train_df.select_dtypes(include=['object']).columns\n",
        "        label_encoders = {}\n",
        "\n",
        "        for col in categorical_cols_eval:\n",
        "            le = LabelEncoder()\n",
        "            le.fit(all_data[col].astype(str))\n",
        "            label_encoders[col] = le\n",
        "\n",
        "        train_ml = train_df.copy()\n",
        "        test_ml = test_df.copy()\n",
        "        synthetic_ml = synthetic_df.copy()\n",
        "\n",
        "        for col in categorical_cols_eval:\n",
        "            train_ml[col] = label_encoders[col].transform(train_df[col].astype(str))\n",
        "            test_ml[col] = label_encoders[col].transform(test_df[col].astype(str))\n",
        "            synthetic_ml[col] = label_encoders[col].transform(synthetic_df[col].astype(str))\n",
        "\n",
        "        return train_ml, test_ml, synthetic_ml\n",
        "\n",
        "    original_train_ml, original_test_ml, synthetic_ml = prepare_ml_data_consistent(\n",
        "        original_train_data, original_test_data, synthetic_data\n",
        "    )\n",
        "\n",
        "    X_train_orig = original_train_ml.drop('income', axis=1)\n",
        "    y_train_orig = original_train_ml['income']\n",
        "    X_test_orig = original_test_ml.drop('income', axis=1)\n",
        "    y_test_orig = original_test_ml['income']\n",
        "\n",
        "    rf_baseline = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "    rf_baseline.fit(X_train_orig, y_train_orig)\n",
        "    baseline_auc = roc_auc_score(y_test_orig, rf_baseline.predict_proba(X_test_orig)[:, 1])\n",
        "\n",
        "    X_train_synth = synthetic_ml.drop('income', axis=1)\n",
        "    y_train_synth = synthetic_ml['income']\n",
        "\n",
        "    X_train_synth = X_train_synth[X_train_orig.columns]\n",
        "\n",
        "    rf_synthetic = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "    rf_synthetic.fit(X_train_synth, y_train_synth)\n",
        "    synthetic_auc = roc_auc_score(y_test_orig, rf_synthetic.predict_proba(X_test_orig)[:, 1])\n",
        "\n",
        "    efficacy_ratio = synthetic_auc / baseline_auc\n",
        "\n",
        "    return baseline_auc, synthetic_auc, efficacy_ratio"
      ],
      "metadata": {
        "id": "ksiEEP979sL3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation\n"
      ],
      "metadata": {
        "id": "AK4lfFuS9v3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_train = data_clean.iloc[:len(X_train)].reset_index(drop=True)\n",
        "original_test = data_clean.iloc[len(X_train):].reset_index(drop=True)\n",
        "\n",
        "print(\"Running Regularized Model Evaluation...\")\n",
        "reg_detection_auc_reg, _ = detection_evaluation_4fold(original_train, regularized_dataset1_final)\n",
        "reg_detection_auc_cond, _ = detection_evaluation_4fold(original_train, regularized_dataset2_final)\n",
        "\n",
        "print(\"Running Regularized Efficacy Evaluation...\")\n",
        "reg_baseline_auc_reg, reg_synthetic_auc_reg, reg_efficacy_ratio_reg = efficacy_evaluation(original_train, original_test, regularized_dataset1_final)\n",
        "reg_baseline_auc_cond, reg_synthetic_auc_cond, reg_efficacy_ratio_cond = efficacy_evaluation(original_train, original_test, regularized_dataset2_final)"
      ],
      "metadata": {
        "id": "tiglxNMP9wgI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2c5f5a8-79b0-41b8-cc06-2c996a895a5e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Regularized Model Evaluation...\n",
            "Running Regularized Efficacy Evaluation...\n"
          ]
        }
      ]
    }
  ]
}